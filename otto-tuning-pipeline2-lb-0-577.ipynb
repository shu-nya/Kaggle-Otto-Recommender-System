{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550b5e5a",
   "metadata": {
    "papermill": {
     "duration": 0.00635,
     "end_time": "2023-01-25T05:44:19.573796",
     "exception": false,
     "start_time": "2023-01-25T05:44:19.567446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300385a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:44:19.585492Z",
     "iopub.status.busy": "2023-01-25T05:44:19.584761Z",
     "iopub.status.idle": "2023-01-25T05:44:19.595877Z",
     "shell.execute_reply": "2023-01-25T05:44:19.595081Z"
    },
    "papermill": {
     "duration": 0.019322,
     "end_time": "2023-01-25T05:44:19.598069",
     "exception": false,
     "start_time": "2023-01-25T05:44:19.578747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Balance of type weighting タイプごとの重み付けバランス\n",
    "# 0:clicks 1:carts 2:orders\n",
    "type_weight = {0:0.5,\n",
    "               1:9,\n",
    "               2:0.5}\n",
    "type_weight_multipliers = type_weight\n",
    "\n",
    "# Use top X for clicks, carts and orders Top何位までを使うか\n",
    "clicks_th = 15 # クリック数\n",
    "carts_th  = 20 # カート数\n",
    "orders_th = 20 # 購入数\n",
    "\n",
    "VER = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a48189",
   "metadata": {
    "papermill": {
     "duration": 0.00443,
     "end_time": "2023-01-25T05:44:19.607183",
     "exception": false,
     "start_time": "2023-01-25T05:44:19.602753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The following is an appropriation of CHRIS DEOTTE's notebook. Thank you!\n",
    "### https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca8d25",
   "metadata": {
    "papermill": {
     "duration": 0.004609,
     "end_time": "2023-01-25T05:44:19.616571",
     "exception": false,
     "start_time": "2023-01-25T05:44:19.611962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Candidate ReRank Model using Handcrafted Rules\n",
    "In this notebook, we present a \"candidate rerank\" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.\n",
    "\n",
    " UPDATE: I published a notebook to compute validation score [here][10] using Radek's scheme described [here][11].\n",
    "\n",
    "Note in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n",
    "\n",
    "### Step 1 - Generate Candidates\n",
    "For each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n",
    "* User history of clicks, carts, orders\n",
    "* Most popular 20 clicks, carts, orders during test week\n",
    "* Co-visitation matrix of click/cart/order to cart/order with type weighting\n",
    "* Co-visitation matrix of cart/order to cart/order called buy2buy\n",
    "* Co-visitation matrix of click/cart/order to clicks with time weighting\n",
    "\n",
    "### Step 2 - ReRank and Choose 20\n",
    "Given the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n",
    "* Most recent previously visited items\n",
    "* Items previously visited multiple times\n",
    "* Items previously in cart or order\n",
    "* Co-visitation matrix of cart/order to cart/order\n",
    "* Current popular items\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n",
    "  \n",
    "# Credits\n",
    "We thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]\n",
    "\n",
    "[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n",
    "[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n",
    "[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n",
    "[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n",
    "[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n",
    "[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n",
    "[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n",
    "[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n",
    "[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n",
    "[10]: https://www.kaggle.com/cdeotte/compute-validation-score-cv-564\n",
    "[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da1e09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004526,
     "end_time": "2023-01-25T05:44:19.625876",
     "exception": false,
     "start_time": "2023-01-25T05:44:19.621350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Candidate Generation with RAPIDS\n",
    "For candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69f4117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:44:19.636835Z",
     "iopub.status.busy": "2023-01-25T05:44:19.636042Z",
     "iopub.status.idle": "2023-01-25T05:44:22.332950Z",
     "shell.execute_reply": "2023-01-25T05:44:22.331784Z"
    },
    "papermill": {
     "duration": 2.704923,
     "end_time": "2023-01-25T05:44:22.335529",
     "exception": false,
     "start_time": "2023-01-25T05:44:19.630606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 21.10.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8ecee",
   "metadata": {
    "papermill": {
     "duration": 0.004684,
     "end_time": "2023-01-25T05:44:22.345230",
     "exception": false,
     "start_time": "2023-01-25T05:44:22.340546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Three Co-visitation Matrices with RAPIDS\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77772cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:44:22.356490Z",
     "iopub.status.busy": "2023-01-25T05:44:22.355678Z",
     "iopub.status.idle": "2023-01-25T05:45:23.794395Z",
     "shell.execute_reply": "2023-01-25T05:45:23.793227Z"
    },
    "papermill": {
     "duration": 61.451751,
     "end_time": "2023-01-25T05:45:23.801716",
     "exception": false,
     "start_time": "2023-01-25T05:44:22.349965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 25.\n",
      "CPU times: user 47.2 s, sys: 8.7 s, total: 55.9 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b39b9",
   "metadata": {
    "papermill": {
     "duration": 0.004872,
     "end_time": "2023-01-25T05:45:23.812334",
     "exception": false,
     "start_time": "2023-01-25T05:45:23.807462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9435dabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:45:23.823586Z",
     "iopub.status.busy": "2023-01-25T05:45:23.823244Z",
     "iopub.status.idle": "2023-01-25T05:48:46.285458Z",
     "shell.execute_reply": "2023-01-25T05:48:46.284275Z"
    },
    "papermill": {
     "duration": 202.470936,
     "end_time": "2023-01-25T05:48:46.288272",
     "exception": false,
     "start_time": "2023-01-25T05:45:23.817336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 9s, sys: 1min 10s, total: 3min 20s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<carts_th].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce9cc1",
   "metadata": {
    "papermill": {
     "duration": 0.011919,
     "end_time": "2023-01-25T05:48:46.312797",
     "exception": false,
     "start_time": "2023-01-25T05:48:46.300878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6455339c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:48:46.338747Z",
     "iopub.status.busy": "2023-01-25T05:48:46.338445Z",
     "iopub.status.idle": "2023-01-25T05:49:17.908069Z",
     "shell.execute_reply": "2023-01-25T05:49:17.902518Z"
    },
    "papermill": {
     "duration": 31.602334,
     "end_time": "2023-01-25T05:49:17.927680",
     "exception": false,
     "start_time": "2023-01-25T05:48:46.325346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 21.7 s, sys: 9.35 s, total: 31 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            \n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "\n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<orders_th].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1e21c",
   "metadata": {
    "papermill": {
     "duration": 0.028414,
     "end_time": "2023-01-25T05:49:17.979461",
     "exception": false,
     "start_time": "2023-01-25T05:49:17.951047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2d5e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:49:18.013851Z",
     "iopub.status.busy": "2023-01-25T05:49:18.013526Z",
     "iopub.status.idle": "2023-01-25T05:52:34.070560Z",
     "shell.execute_reply": "2023-01-25T05:52:34.069027Z"
    },
    "papermill": {
     "duration": 196.074391,
     "end_time": "2023-01-25T05:52:34.072966",
     "exception": false,
     "start_time": "2023-01-25T05:49:17.998575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 6s, sys: 1min 8s, total: 3min 15s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            # 1659304800 : minimum timestamp\n",
    "            # 1662328791 : maximum timestamp\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<clicks_th].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce412ac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:52:34.117564Z",
     "iopub.status.busy": "2023-01-25T05:52:34.116496Z",
     "iopub.status.idle": "2023-01-25T05:52:34.268578Z",
     "shell.execute_reply": "2023-01-25T05:52:34.267597Z"
    },
    "papermill": {
     "duration": 0.176605,
     "end_time": "2023-01-25T05:52:34.270972",
     "exception": false,
     "start_time": "2023-01-25T05:52:34.094367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90e488",
   "metadata": {
    "papermill": {
     "duration": 0.020787,
     "end_time": "2023-01-25T05:52:34.313213",
     "exception": false,
     "start_time": "2023-01-25T05:52:34.292426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - ReRank (choose 20) using handcrafted rules\n",
    "For description of the handcrafted rules, read this notebook's intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9dfeff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:52:34.356895Z",
     "iopub.status.busy": "2023-01-25T05:52:34.356553Z",
     "iopub.status.idle": "2023-01-25T05:52:35.860650Z",
     "shell.execute_reply": "2023-01-25T05:52:35.859607Z"
    },
    "papermill": {
     "duration": 1.52878,
     "end_time": "2023-01-25T05:52:35.863294",
     "exception": false,
     "start_time": "2023-01-25T05:52:34.334514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (6928123, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13099779</td>\n",
       "      <td>972319</td>\n",
       "      <td>1661795888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13099779</td>\n",
       "      <td>972319</td>\n",
       "      <td>1661795898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aid          ts  type\n",
       "0  13099779  245308  1661795832     0\n",
       "1  13099779  245308  1661795862     1\n",
       "2  13099779  972319  1661795888     0\n",
       "3  13099779  972319  1661795898     1\n",
       "4  13099779  245308  1661795907     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93d37ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:52:35.931917Z",
     "iopub.status.busy": "2023-01-25T05:52:35.931422Z",
     "iopub.status.idle": "2023-01-25T05:54:19.351001Z",
     "shell.execute_reply": "2023-01-25T05:54:19.349998Z"
    },
    "papermill": {
     "duration": 103.47726,
     "end_time": "2023-01-25T05:54:19.374151",
     "exception": false,
     "start_time": "2023-01-25T05:52:35.896891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are size of our 3 co-visitation matrices:\n",
      "1837166 1168768 1837166\n",
      "CPU times: user 1min 41s, sys: 4.91 s, total: 1min 46s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
    "\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n",
    "\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n",
    "\n",
    "\n",
    "top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n",
    "\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n",
    "\n",
    "top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "#top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n",
    "#top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abcd623e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:54:19.418509Z",
     "iopub.status.busy": "2023-01-25T05:54:19.418200Z",
     "iopub.status.idle": "2023-01-25T05:54:19.767039Z",
     "shell.execute_reply": "2023-01-25T05:54:19.765955Z"
    },
    "papermill": {
     "duration": 0.373788,
     "end_time": "2023-01-25T05:54:19.769438",
     "exception": false,
     "start_time": "2023-01-25T05:54:19.395650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_clicks = test_df.loc[test_df['type']== 0,'aid'].value_counts().index.values[:20] \n",
    "top_carts = test_df.loc[test_df['type']== 1,'aid'].value_counts().index.values[:20]\n",
    "top_orders = test_df.loc[test_df['type']== 2,'aid'].value_counts().index.values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "838ba9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:54:19.814129Z",
     "iopub.status.busy": "2023-01-25T05:54:19.813812Z",
     "iopub.status.idle": "2023-01-25T05:54:19.853292Z",
     "shell.execute_reply": "2023-01-25T05:54:19.852346Z"
    },
    "papermill": {
     "duration": 0.063849,
     "end_time": "2023-01-25T05:54:19.855197",
     "exception": false,
     "start_time": "2023-01-25T05:54:19.791348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def suggest_clicks(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=20:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb2a616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:54:19.901479Z",
     "iopub.status.busy": "2023-01-25T05:54:19.899853Z",
     "iopub.status.idle": "2023-01-25T05:54:19.911578Z",
     "shell.execute_reply": "2023-01-25T05:54:19.910639Z"
    },
    "papermill": {
     "duration": 0.036933,
     "end_time": "2023-01-25T05:54:19.913617",
     "exception": false,
     "start_time": "2023-01-25T05:54:19.876684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def suggest_carts(df):\n",
    "    # User history aids and types\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    \n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type'] == 0)|(df['type'] == 1)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    \n",
    "    # Rerank candidates using weights\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        \n",
    "        # Rerank based on repeat items and types of items\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        \n",
    "        # Rerank candidates using\"top_20_carts\" co-visitation matrix\n",
    "        aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))\n",
    "        for aid in aids2: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    \n",
    "    # Use \"cart order\" and \"clicks\" co-visitation matrices\n",
    "    aids1 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    \n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids1+aids2).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_carts)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe24a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:54:19.958510Z",
     "iopub.status.busy": "2023-01-25T05:54:19.957738Z",
     "iopub.status.idle": "2023-01-25T05:54:19.968154Z",
     "shell.execute_reply": "2023-01-25T05:54:19.967271Z"
    },
    "papermill": {
     "duration": 0.034736,
     "end_time": "2023-01-25T05:54:19.970089",
     "exception": false,
     "start_time": "2023-01-25T05:54:19.935353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def suggest_buys(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76649f8",
   "metadata": {
    "papermill": {
     "duration": 0.021232,
     "end_time": "2023-01-25T05:54:20.012476",
     "exception": false,
     "start_time": "2023-01-25T05:54:19.991244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV\n",
    "Inferring test data with Pandas groupby is slow. We need to accelerate the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d68fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T05:54:20.057694Z",
     "iopub.status.busy": "2023-01-25T05:54:20.056743Z",
     "iopub.status.idle": "2023-01-25T06:46:10.673053Z",
     "shell.execute_reply": "2023-01-25T06:46:10.671942Z"
    },
    "papermill": {
     "duration": 3110.664446,
     "end_time": "2023-01-25T06:46:10.698255",
     "exception": false,
     "start_time": "2023-01-25T05:54:20.033809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51min 28s, sys: 18.4 s, total: 51min 47s\n",
      "Wall time: 51min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")\n",
    "\n",
    "pred_df_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_carts(x)\n",
    ")\n",
    "\n",
    "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f0b3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T06:46:10.744793Z",
     "iopub.status.busy": "2023-01-25T06:46:10.744096Z",
     "iopub.status.idle": "2023-01-25T06:46:14.587561Z",
     "shell.execute_reply": "2023-01-25T06:46:14.586432Z"
    },
    "papermill": {
     "duration": 3.8691,
     "end_time": "2023-01-25T06:46:14.590007",
     "exception": false,
     "start_time": "2023-01-25T06:46:10.720907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0a1be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T06:46:14.635641Z",
     "iopub.status.busy": "2023-01-25T06:46:14.635325Z",
     "iopub.status.idle": "2023-01-25T06:46:59.598197Z",
     "shell.execute_reply": "2023-01-25T06:46:59.597278Z"
    },
    "papermill": {
     "duration": 45.009419,
     "end_time": "2023-01-25T06:46:59.621762",
     "exception": false,
     "start_time": "2023-01-25T06:46:14.612343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>59625 1253524 737445 438191 731692 1790770 942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>1142000 736515 973453 582732 889686 487136 141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>918667 199008 194067 57315 141736 1460571 7594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>834354 740494 987399 889671 779477 127404 1711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>1817895 607638 1754419 1216820 1729553 300127 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  12899779_clicks  59625 1253524 737445 438191 731692 1790770 942...\n",
       "1  12899780_clicks  1142000 736515 973453 582732 889686 487136 141...\n",
       "2  12899781_clicks  918667 199008 194067 57315 141736 1460571 7594...\n",
       "3  12899782_clicks  834354 740494 987399 889671 779477 127404 1711...\n",
       "4  12899783_clicks  1817895 607638 1754419 1216820 1729553 300127 ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3771.714084,
   "end_time": "2023-01-25T06:47:02.872353",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-25T05:44:11.158269",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
